---
title: "Project 4: Portfolio Optimization"
output:
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
runtime: shiny
---

```{r setup, include=FALSE}
# global reads, processing
library(ggplot2)
library(gridExtra)
library(grid)
library(flexdashboard)
library(shiny)
library(QRM) #GPD fit
library(qrmdata)
library(xts)
library(zoo)
library(psych)
library(quadprog)
library(matrixStats)
library(quantreg)
library(moments)
library(plotly)
library(mvtnorm)
library(plotly)
library(reshape2)

#########################################################
#
# Exploratory Analysis
#
#########################################################
#Removes all objects in this environment to blank out for the dashboard
rm(list = ls())
#Reading in the data
data <- na.omit(read.csv(url("https://turing.manhattan.edu/~wfoote01/finalytics/data/metaldata.csv"), header = TRUE))
#Applies the log difference to the numeric columns in the data matrix and multiplies by 100 to get percentage
data.r <- apply(log(data[,-1]), 2, diff) * 100
#Finding the magnitude of the percent change with absolute value
size <- na.omit(abs(data.r))

#adds .size to the end of the column
colnames(size) <- paste(colnames(size), ".size", sep = "")
#If the returns is positive, make a new matrix have a 1 in that place
#If the returns is negative, put a -1 in that place, otherwise a 0
direction <- ifelse(data.r > 0, 1, ifelse(data.r < 0, -1, 0))
#Adding .dir to the end of the column names in the dirextion data frame
colnames(direction) <- paste(colnames(direction), ".dir", sep = "")
#Making a vector of the dates
dates <- as.Date(data[-1,1], "%m/%d/%Y")
#Making dates characters
dates.chr <- as.character(dates)
#Combining all columns to make a values data frame
values <- cbind(data.r, size, direction)
#Creating tidy data frames
data.df <- data.frame(dates = dates, returns = data.r, size = size, direction = direction)
data.df.nd <- data.frame(dates = dates.chr, returns = data.r, size = size, direction = direction, stringsAsFactors = FALSE)

#Creating a time series object
data.xts <- na.omit(as.xts(values, dates))
#Making a zoo object
data.zr <- as.zooreg(data.xts)
returns <- data.xts
```

Business Questions
=======================================================================

Column {.tabset}
-----------------------------------------------------------------------

### Freight Forwarder

A freight forwarder with a fleet of bulk carriers wants to optimize their portfolio in the metals markets with entry into the nickel business and use of the tramp trade.  Tramp ships are the company's "swing" option without any fixed charter or other constraint. They allow the company flexibility in managing several aspects of freight uncertainty.   They have allocated \$250 million to purchase metals. The company wants us to:

1.	Retrieve and begin to analyze data about potential commodities to diversify into
2.	Compare potential commodities with existing commodities in conventional metals spot markets
3.	Begin to generate economic scenarios based on events that may, or may not, materialize in the commodities
4.	The company wants to mitigate their risk by diversifying their cargo loads

Identify the optimal combination of Nickel, Copper, and Aluminium to trade

1.	Product: Metals commodities and freight charters
2.	Metal, Company, and Geography:
    a. Nickel: MMC Norilisk, Russia
    b. Copper: Codelco, Chile and MMC Norilisk, Russia
    c. Aluminium: Vale, Brasil and Rio Tinto Alcan, Australia
3.	Customers: Ship Owners, manufacturers, traders
4.  All metals traded on the London Metal Exchange 

### Key business questions

The purpose of this exercise is to come up with positions that would best suit our freight trade, if we were to enter  the nickel market. Some of the key business qustions we would need to answer to fully evaluate this scenario are:

####Distribution of Returns

Does entering into nickel sales increase risk or decrease returns?

  * To understand how selling nickel will accect our returns, we will want to look at historical data, and see what re turns would look like if we had also been selling nickel instead of only copper and aluminum. We also want to look at the returns and standard deviation on nickel as compared to the other metals as well. 

####Optimized Portfolio

If we were to start selling nickel, how should we position our portfolio?

  * Using the historical prices for all the metals, we create many scenarios for target returns and solve for the portfolio that would meet those target returns. After solving for returns and risk, as measured by standard deviation, we want to map the efficient frontier, from the point of minimum risk, all up to the scenarion with the maximum sharpe's ratio, or the ratio of returns to risk, taking into account our risk-free asset. We would want to choose a portfolio somewhere between the portfolio we created with minimum variation, and the portfolio that resulted in the maximum sharpe's ratio, depending on how much risk our company is willing to take on.

####No Short Positions

If we only want to have long positions, how would we set up our portfolio?

  * If our company does not want to rely on taking a short position, we would need to evaluate scenarios in which we constrain our equation to only allow positions greater than or equal to zero. This would mean our only business is selling metals, but it could mean we do not take a position on all of the metals (0%). 

####Range of Returns

Using a simulated model, can we come up with a likely range of returns if we were to take the position that maximizes the sharpe's ratio?

  * We know that the data we optimized our portfolio against is likely to not occur again, but it is likely that the distribution of returns in the future (at least near future) will have a similar distribution. Can create a range of expected returns by using a simulated historical returns based on the distribution of returns from our actual returns, and using this optimized weight from the simulation, see what this position would have actually returned on the actual historical data?


Metals Distribution
=======================================================================
Column
----
### Distribution Statistics
```{r}
#Removes all objects in this environment to blank out for the dashboard
rm(list = ls())
#Reading in the data
data <- na.omit(read.csv(url("https://turing.manhattan.edu/~wfoote01/finalytics/data/metaldata.csv"), header = TRUE))
#Applies the log difference to the numeric columns in the data matrix and multiplies by 100 to get percentage
data.r <- apply(log(data[,-1]), 2, diff) * 100
#Finding the magnitude of the percent change with absolute value
size <- na.omit(abs(data.r))

#adds .size to the end of the column
colnames(size) <- paste(colnames(size), ".size", sep = "")
#If the returns is positive, make a new matrix have a 1 in that place
#If the returns is negative, put a -1 in that place, otherwise a 0
direction <- ifelse(data.r > 0, 1, ifelse(data.r < 0, -1, 0))
#Adding .dir to the end of the column names in the dirextion data frame
colnames(direction) <- paste(colnames(direction), ".dir", sep = "")
#Making a vector of the dates
dates <- as.Date(data[-1,1], "%m/%d/%Y")
#Making dates characters
dates.chr <- as.character(dates)
#Combining all columns to make a values data frame
values <- cbind(data.r, size, direction)
#Creating tidy data frames
data.df <- data.frame(dates = dates, returns = data.r, size = size, direction = direction)
data.df.nd <- data.frame(dates = dates.chr, returns = data.r, size = size, direction = direction, stringsAsFactors = FALSE)

#Creating a time series object
data.xts <- na.omit(as.xts(values, dates))
#Making a zoo object
data.zr <- as.zooreg(data.xts)
returns <- data.xts

#Data Moments for nickel and 
data_moments <- function(data) {
    library(moments)
    library(matrixStats)
    mean.r <- colMeans(data)
    median.r <- colMedians(data)
    sd.r <- colSds(data)
    IQR.r <- colIQRs(data)
    skewness.r <- skewness(data)
    kurtosis.r <- kurtosis(data)
    result <- data.frame(mean = mean.r, 
        median = median.r, std_dev = sd.r, 
        IQR = IQR.r, skewness = skewness.r, 
        kurtosis = kurtosis.r)
    return(result)
}
# Run data moments for the returns and sizes of all metals
answer <- data_moments(data.xts[, 1:3])
# Build pretty table with 4 digits
answer <- round(answer, 4)
#Knitting the table
knitr::kable(answer)
```


To fully understand the metal returns, we need to look at the distribution statistics for each of returns. These distribution statistics will descibe the central tendency of the returns, the dispersion of the returns, and the shape of the returns. The statistics that measure central tendency, or the typical behavior are mean and median. Statistics that describe dispersion are IQR and standard deviation. Statistics that describe the shape of the returns are kurtosis and skewness.

####Nickel
Nickel has a mean and median slightly greater than zero, but a fairly IQR and standard deviation. This means that there is fairly large risk in nickel. The kurtosis is also larger than a normal distribution, which means there are more events in the tail than a normal distribution. Fortunately, the skew is positive, which means the positive side of the tail is more likely than the negative side of the tail.

####Copper
Copper has a mean and median slightly greater than zero, but not as large as the nickel mean and median. The IQR is not nearly as large as the nickel, and standard deviation is significantly smaller. This means there is slightly less risk in copper than nickel. The kurtosis is also slightly larger than a normal distribution, which means there are more events in the tail than a normal distribution. Unfortunately, the skew is negative, which means the negative side of the tail is more likely than the positive side of the tail.

####Aluminum
Aluminum has a mean and median slightly greater than zero, but not as large as the nickel or copper mean and median. The IQR is quite narrow as is the standard deviation. This means there is little risk in aluminum. The kurtosis is larger than a normal distribution, which means there are more events in the tail than a normal distribution. Unfortunately, the skew is negative, which means the negative side of the tail is more likely than the positive side of the tail.

Efficient Frontier {data-orientation=rows}
====
Input {.sidebar data-orientation=rows}
-----------------------------------------------------------------------
When evaluating portfolio optimization, we need to decide whether we want to optimize our portfolio for the best overall position, or if we do not want to allow any short positions (meaning we are only selling). The selection below changes which scenario we are evaluating.

```{r}
selectInput("selection", label = "Portfolio Type:",
            choices = c("All Positions", "No Short Position"), selected = "All Positions")
```


To optimize the portfolio, one needs to determine what they believe to be realitic targets for returns. The slider below will adjust the quantile of nickel returns that you wish to base your analysis off of. The days indexed for all metals will be when nickel returns are greater than the quantile specified.

```{r}
sliderInput("alpha_nickel", label = "Quantile for Nickel Returns to Evaluate:",
            min = 0.75, max = 0.99, value = 0.75, step = 0.01)
```

The sharpe ratio is also based off of the rate of return on a risk-free asset. Below is a slider that will adjust the returns for that risk free asset.

```{r}
sliderInput("riskfree", label = "Percent Annual Returns for Risk Free Asset:",
            min = 0.5, max = 10, value = 0.5, step = 0.25)
```

Row {data-height=850}
-----------------------------------------------------------------------
###Efficient Frontier
```{r}
#Storing the returns in the variable R and taking them out of 
#percentage for, for calculations
R <- returns[,1:3]/100
quantile_R <- quantile(R[, 1], 0.95)         
R <- subset(R, nickel > quantile_R, select = nickel:aluminium)
#Slider for the percentage of nicel returns
alpha <- reactive({ifelse(input$alpha_nickel>1,0.99,ifelse(input$alpha_nickel<0,0.001,input$alpha_nickel))})
riskfree <- reactive({ifelse(input$riskfree>10,10,ifelse(input$riskfree<0,0.25,input$riskfree))})
renderPlotly({

#getting the tails of the nickel distribution
quantile_R <- quantile(R[, 1], alpha())               
#Getting all columns when nickel is at it's extreme
R <- subset(R, nickel > quantile_R, select = nickel:aluminium)
#Getting the names for the columns
names.R <- colnames(R)
#Applying the mean for the returns
mean.R <- apply(R, 2, mean)
#Creating a covariance matrix to extract the diagonals
cov.R <- cov(R)
#Diagonals are the variances, taking the sqrt give stds
sd.R <- sqrt(diag(cov.R)) 
if (input$selection == "All Positions"){
#Ensuring that the weights add up to 1
Amat <- cbind(rep(1, 3), mean.R)
#Creating a sequence of targets returns from half minimum avg returns
#To 150% of the maximum average returns with 300 possibilities
mu.P <- seq(0.5 * min(mean.R), 1.5 * 
    max(mean.R), length = 300)  
#Variable to store stdevs
sigma.P <- mu.P
#Creatinf an empty matrix to hold the various positions of the metals
weights <- matrix(0, nrow = 300, ncol = ncol(R))
#Giving the weights a column name
colnames(weights) <- names.R
#For each of the target returns
for (i in 1:length(mu.P)) {
    #saving the constraint vector for that scenario
    bvec <- c(1, mu.P[i])
    #determining the solutions for that target allowing a short position
    result <- solve.QP(Dmat = 2 * cov.R, 
        dvec = rep(0, 3), Amat = Amat, 
        bvec = bvec, meq = 2)
    #Saving the stdevs in the sigma.P variable in index of this iteration
    sigma.P[i] <- sqrt(result$value)
    #Saving the weights in the row index of this iteration
    weights[i, ] <- result$solution
}
#creating a data frame with the stdevs and the target
sigma.mu.df <- data.frame(sigma.P = sigma.P, 
    mu.P = mu.P)
#Getting the daily return for a risk free assest
#30 yr treasury yield (3% / 100 / 365)
mu.free <- (riskfree() / 100 / 365)
#Finding the sharpe ration to maximize the return to risk
sharpe <- (mu.P - mu.free)/sigma.P
#Finding indices of where the ratio is the largest
ind <- (sharpe == max(sharpe))
#index where variance is smallest
ind2 <- (sigma.P == min(sigma.P))
#Finding all indices where ratio is larger than the smallest variance
ind3 <- (mu.P > mu.P[ind2])
#creating a vector that gives a color:
#blue if returns are greater than the returns at minimum variance
#grey if returns are less than returns at minimum variance making 
#effiviant frontier blue
col.P <- ifelse(mu.P > mu.P[ind2], "blue", 
    "grey")
#adding color ot the data frame of stds and returns
sigma.mu.df$col.P <- col.P
# plotting Efficient Frontier x is variance, y is target returns
#Efficient frontier plotted as blue in the line plot
p <- ggplot(sigma.mu.df, aes(x = sigma.P, 
    y = mu.P, group = 1)) + geom_line(aes(colour = col.P, 
    group = col.P)) + scale_colour_identity()
#adding a red dot representing the risk-free asset
p <- p + geom_point(aes(x = 0, y = mu.free), 
    colour = "red")
options(digits = 4)
#Drawing line from risk free asset to the max sharpe ratio
#Creating a line tangent to the curve
p <- p + geom_abline(intercept = mu.free, 
    slope = (mu.P[ind] - mu.free)/sigma.P[ind], 
    colour = "red")
#Adding a point to the maximum sharpe ratio
p <- p + geom_point(aes(x = sigma.P[ind], 
    y = mu.P[ind]))
#Adding a point at minimum variance
p <- p + geom_point(aes(x = sigma.P[ind2], 
    y = mu.P[ind2]))
#Adding text of the metal to the mean and stdev of returns for 
#aluminum, copper, and nickel
p <- p + annotate("text", x = sd.R[1], 
    y = mean.R[1], label = names.R[1]) + 
    annotate("text", x = sd.R[2], y = mean.R[2], 
        label = names.R[2]) + annotate("text", 
    x = sd.R[3], y = mean.R[3], label = names.R[3])
# Showing the interactive graph 
ggplotly(p)}
else{
#creating constrint vector with 1's as the intercept, means as the weight
#and the diagonal of 1's as teh position
Amat <-  cbind(rep(1,3),mean.R,diag(1,nrow=3))
length.P <- 300
#Creating a sequence of targets returns from half minimum avg returns
#To 150% of the maximum average returns with 300 possibilities
mu.P <-  seq(min(mean.R)+.0001,max(mean.R)-.0001,length = length.P) 
# variable to hold the stdevs
sigma.P <- mu.P
#Variable to hold the weights
weights <-  matrix(0, nrow = length.P, ncol = 3)
#Giving the weights a column name
colnames(weights) <- names.R
#For each potential returns scenario
for (i in 1:length(mu.P))
{
  #vector constraining no short postions
  bvec <-  c(1,mu.P[i],rep(0,3))
  #Finding the portfolio that meets the returns and calculating the stdevs
  result <-  
    solve.QP(Dmat=2*cov.R,dvec=rep(0,3),Amat=Amat,bvec=bvec,meq=2)
  sigma.P[i] <-  sqrt(result$value)
  weights[i,] <-  result$solution
}
#Saving a data frame of the stdevs and the target returns
sigma.mu.df <- data.frame(sigma.P = sigma.P, mu.P = mu.P )
#Getting the daily return for a risk free assest
#30 yr treasury yield (3% / 100 / 365)
mu.free <- (riskfree() / 100 / 365)
#Finding the sharpe's ratio for every scenario
sharpe <- ( mu.P-mu.free)/sigma.P
#indexing max sharpe's ratio
ind <-  (sharpe == max(sharpe))
#Indexing the minimum variance
ind2 <-  (sigma.P == min(sigma.P))
#indexing where the returns are greater than the returns at
#the minimum variance position
ind3 <-  (mu.P > mu.P[ind2])
#Creating a column for colors that show when the plot is
#greater than the minimum variance
col.P <- ifelse(mu.P > mu.P[ind2], "blue", "grey")
#Adding the colors to teh dataframe for plotting
sigma.mu.df$col.P <- col.P
#when flexdashboard renderPlotly({
p <- ggplot(sigma.mu.df, aes(x = sigma.P, y = mu.P, group = 1)) + geom_line(aes(colour=col.P, group = col.P)) + scale_colour_identity() # + xlim(0, max(sd.R*1.1))  + ylim(0, max(mean.R)*1.1) + 
p <- p + geom_point(aes(x = 0, y = mu.free), colour = "red")
options(digits=4)
p <- p + geom_abline(intercept = mu.free, slope = (mu.P[ind]-mu.free)/sigma.P[ind], colour = "red")
p <- p + geom_point(aes(x = sigma.P[ind], y = mu.P[ind])) 
p <- p + geom_point(aes(x = sigma.P[ind2], y = mu.P[ind2])) ## show min var portfolio
p <- p + annotate("text", x = sd.R[1], y = mean.R[1], label = names.R[1]) + annotate("text", x = sd.R[2], y = mean.R[2], label = names.R[2]) + annotate("text", x = sd.R[3], y = mean.R[3], label = names.R[3])
p <- p + geom_vline(aes(xintercept = sd.R[2]), color = "red")
ggplotly(p)  
}
}
)
```

Row
---
###Metals Positions

```{r}
#Storing the returns in the variable R and taking them out of 
#percentage for, for calculations
R <- returns[, 1:3]/100
#getting the tails of the nickel distribution
renderTable({
quantile_R <- quantile(R[, 1], alpha())               
#Getting all columns when nickel is at it's extreme
R <- subset(R, nickel > quantile_R, select = nickel:aluminium)
#Getting the names for the columns
names.R <- colnames(R)
#Applying the mean for the returns
mean.R <- apply(R, 2, mean)
#Creating a covariance matrix to extract the diagonals
cov.R <- cov(R)
#Diagonals are the variances, taking the sqrt give stds
sd.R <- sqrt(diag(cov.R))
if (input$selection == "All Positions"){
#Ensuring that the weights add up to 1
Amat <- cbind(rep(1, 3), mean.R)
#Creating a sequence of targets returns from half minimum avg returns
#To 150% of the maximum average returns with 300 possibilities
mu.P <- seq(0.5 * min(mean.R), 1.5 * 
    max(mean.R), length = 300)  
#Variable to store stdevs
sigma.P <- mu.P
#Creatinf an empty matrix to hold the various positions of the metals
weights <- matrix(0, nrow = 300, ncol = ncol(R))
#Giving the weights a column name
colnames(weights) <- names.R
#For each of the target returns
for (i in 1:length(mu.P)) {
    #saving the constraint vector for that scenario
    bvec <- c(1, mu.P[i])
    #determining the solutions for that target allowing a short position
    result <- solve.QP(Dmat = 2 * cov.R, 
        dvec = rep(0, 3), Amat = Amat, 
        bvec = bvec, meq = 2)
    #Saving the stdevs in the sigma.P variable in index of this iteration
    sigma.P[i] <- sqrt(result$value)
    #Saving the weights in the row index of this iteration
    weights[i, ] <- result$solution
}
#creating a data frame with the stdevs and the target
sigma.mu.df <- data.frame(sigma.P = sigma.P, 
    mu.P = mu.P)
#Getting the daily return for a risk free assest
#30 yr treasury yield (3% / 100 / 365)
mu.free <- (riskfree() / 100 / 365)
#Finding the sharpe ration to maximize the return to risk
sharpe <- (mu.P - mu.free)/sigma.P
#Finding indices of where the ratio is the largest
ind <- (sharpe == max(sharpe))
#index where variance is smallest
ind2 <- (sigma.P == min(sigma.P))
#Finding all indices where ratio is larger than the smallest variance
ind3 <- (mu.P > mu.P[ind2])
#creating a vector that gives a color:
#blue if returns are greater than the returns at minimum variance
#grey if returns are less than returns at minimum variance making 
#effiviant frontier blue
col.P <- ifelse(mu.P > mu.P[ind2], "blue", 
    "grey")
#adding color ot the data frame of stds and returns
sigma.mu.df$col.P <- col.P


sharpe.position <- weights[ind,] * 100
names <- names(sharpe.position)
sharpe.position <- paste(round(sharpe.position, 2), "%", sep = "")
names(sharpe.position) <- names
data.frame(Metal = names, Position = sharpe.position)
}
else{
  #creating constrint vector with 1's as the intercept, means as the weight
#and the diagonal of 1's as teh position
Amat <-  cbind(rep(1,3),mean.R,diag(1,nrow=3))
length.P <- 300
#Creating a sequence of targets returns from half minimum avg returns
#To 150% of the maximum average returns with 300 possibilities
mu.P <-  seq(min(mean.R)+.0001,max(mean.R)-.0001,length = length.P) 
# variable to hold the stdevs
sigma.P <- mu.P
#Variable to hold the weights
weights <-  matrix(0, nrow = length.P, ncol = 3)
#Giving the weights a column name
colnames(weights) <- names.R
#For each potential returns scenario
for (i in 1:length(mu.P))
{
  #vector constraining no short postions
  bvec <-  c(1,mu.P[i],rep(0,3))
  #Finding the portfolio that meets the returns and calculating the stdevs
  result <-  
    solve.QP(Dmat=2*cov.R,dvec=rep(0,3),Amat=Amat,bvec=bvec,meq=2)
  sigma.P[i] <-  sqrt(result$value)
  weights[i,] <-  result$solution
}
#Saving a data frame of the stdevs and the target returns
sigma.mu.df <- data.frame(sigma.P = sigma.P, mu.P = mu.P )
#Getting the daily return for a risk free assest
#30 yr treasury yield (3% / 100 / 365)
mu.free <- (riskfree() / 100 / 365)
#Finding the sharpe's ratio for every scenario
sharpe <- ( mu.P-mu.free)/sigma.P
#indexing max sharpe's ratio
ind <-  (sharpe == max(sharpe))
#Indexing the minimum variance
ind2 <-  (sigma.P == min(sigma.P))
#indexing where the returns are greater than the returns at
#the minimum variance position
ind3 <-  (mu.P > mu.P[ind2])
#Creating a column for colors that show when the plot is
#greater than the minimum variance
col.P <- ifelse(mu.P > mu.P[ind2], "blue", "grey")
#Adding the colors to teh dataframe for plotting
sigma.mu.df$col.P <- col.P
#when flexdashboard renderPlotly({
sharpe.position <- weights[ind,] * 100
names <- names(sharpe.position)
sharpe.position <- paste(round(sharpe.position, 2), "%", sep = "")
names(sharpe.position) <- names
data.frame(Metal = names, Position = sharpe.position)
}
})
```

Bootstrapping Sharpe Ratio
====
Column {.sidebar}
---
```{r}
selectInput("position", label = "Portfolio Type:",
            choices = c("All Positions", "No Short Position"), selected = "All Positions")
```
The sharpe ratio is the best measurement for risk adjusted returns. While we have the historical data, and can calculate the sharpe ratio for the various positions, we do not know if the sharpe ratio will remain the same for future returns. However, we can simulate historical data based on the actual distribution, discover the position that maximizes the sharpe ratio, and see what that position's sharpe ratio would actually be on the actual data. This would give us a range of what to expect for our future sharpe ratio. The selection above allows for modeling of all possible positions and no short positions


Column {.tabset}
---
### Bootstrapping All Positions
```{r}
R <- returns[,1:3]/100
quantile_R <- quantile(R[, 1], 0.95)         
R <- subset(R, nickel > quantile_R, select = nickel:aluminium)
# Getting number of rows in our returns
n <- dim(R)[1]
#Getting the number of columns in our returns
N <- dim(R)[2]
#Getting the actual mean of returns from our metals
mean_vect_ACTUAL <- apply(R, 2, mean)
#Creating the actual covariance matrix for metals
cov_mat_ACTUAL <- cov(R)
# Settign up 1000 simulations
n_boot <- 1000
#Creating a matrix of 1's that has 1000 rows and 2 columns
out <- matrix(1, nrow = n_boot, ncol = 2)
#Creating a matrix of 1's that has 1000 rows and same number of columns as metals
mean_out <- matrix(1, nrow = n_boot, ncol = dim(R)[2])
#Setting a seed for repeatable analysis
set.seed(1016)

renderPlotly({
#For each of the simulations
for (i_boot in (1:n_boot)) {
    # generate n-1 random indices multiplied by the number of possible rows
    #and taking the ceiling so the minimum number an index can be is 1
    uniform <- ceiling((n - 1) * runif(n - 
        1))
    #Selecting the random sample using the random indices
    R_boot <- R[uniform, ]  # select random sample
    #getting the simulated metals mean
    mean_vect <- apply(R_boot, 2, mean)
    #Saving the means in the mean matrix
    mean_out[i_boot, ] <- mean_vect
    #Getting the covariance and standard deviations for the simutaltion
    cov_mat <- cov(R_boot)
    sd_vect <- sqrt(diag(cov_mat))
    #Creating the constraint matrix for this simulation
    Amat <- cbind(rep(1, N), mean_vect)  # short sales allowed
    #Creating a variable to store teh standard deviation of the portfolio 
    sd.P <- numeric(300)
    #Getting the returns for the risk-free asset
    mu.free <- (3 / 100 / 365)
    #creating a sequence for possible returns 
    mu.P <- seq(min(mean_vect) + 1e-04, 
        max(mean_vect) - 1e-04, length = 300)  #length.P)
    #Creating a weight matrix to save the positions (300 rows 3 columns)
    weights <- matrix(0, nrow = 300, 
        ncol = N)
    #fro each oof the possible returns
    for (i in 1:length(mu.P)) {
        #initial values
        bvec <- c(1, mu.P[i])  # short sales
        #Solving portfolio for the target return
        result <- solve.QP(Dmat = 2 * 
            cov_mat, dvec = rep(0, N), 
            Amat = Amat, bvec = bvec, 
            meq = 2)
        #Saving the sd in the standard deviation matrix
        sd.P[i] <- sqrt(result$value)
        #Saving the weights in the weights vector
        weights[i, ] <- result$solution
    }
    #Solving for the sharpe ratio of this scenario
    sharpe <- (mu.P - mu.free)/sd.P
    #Indexing the highest sharpe ratio
    ind <- (sharpe == max(sharpe))
    #Saving the maximum sharpe's ratio for this simulation 
    out[i_boot, 1] <- sharpe[ind]
    #Storing the weights for the sharpe's ratio in the 
    #weights matrix
    w.T <- weights[ind, ]
    #Showing how returns would be for this weight in the actual
    sharpe_ACTUAL <- (w.T %*% mean_vect_ACTUAL - 
        mu.free)/sqrt(w.T %*% cov_mat_ACTUAL %*% 
        w.T)
    #Storing the returns for these weights 
    out[i_boot, 2] <- sharpe_ACTUAL
}
#Creating a data frame that shows what we would 
#have made with those weights in that simulation, What
#the simulation predicted we would have made and the difference
#Between the predicted and the actual returns
out_SHORT <- data.frame(actual = out[, 
    2], predicted = out[, 1], residuals = out[, 
    2] - out[, 1])
#Create a data moments for our predicted and actual returns
out_SUMMARY <- data_moments(as.matrix(out_SHORT))
#Make the table nice
#Storing the results of the actual returns
results <- out_SHORT
#Getting the minimum x & y for plotting
min_xy <- min(min(results$actual), min(results$predicted))
#Getting the maximum x & y for plotting
max_xy <- max(max(results$actual), max(results$predicted))
#Melting the results to create a faect plot
plot_melt <- melt(results, id.vars = "predicted")
#Abbing the max x-y and min x-y so facet plots have the same scale
p <- ggplot(results, aes(predicted, actual)) + 
    geom_point() + ggtitle("Actual vs. Predicted Sharpe's Ratios") + 
    geom_quantile(quantiles = c(0.01, 
        0.99)) + geom_quantile(quantiles = 0.5, 
    linetype = "longdash") + geom_density_2d(colour = "red")
ggplotly(p)
})
```

### Bootstrapping No Short Positions

```{r}
R <- returns[,1:3]/100
quantile_R <- quantile(R[, 1], 0.95)         
R <- subset(R, nickel > quantile_R, select = nickel:aluminium)
# Getting number of rows in our returns
n <- dim(R)[1]
#Getting the number of columns in our returns
N <- dim(R)[2]
#Getting the actual mean of returns from our metals
mean_vect_ACTUAL <- apply(R, 2, mean)
#Creating the actual covariance matrix for metals
cov_mat_ACTUAL <- cov(R)
# Settign up 1000 simulations
n_boot <- 1000
#Creating a matrix of 1's that has 1000 rows and 2 columns
out <- matrix(1, nrow = n_boot, ncol = 2)
#Creating a matrix of 1's that has 1000 rows and same number of columns as metals
mean_out <- matrix(1, nrow = n_boot, ncol = dim(R)[2])
#Setting a seed for repeatable analysis
set.seed(1016)

renderPlotly({
#For each of the simulations
for (i_boot in (1:n_boot)) {
    # generate n-1 random indices multiplied by the number of possible rows
    #and taking the ceiling so the minimum number an index can be is 1
    uniform <- ceiling((n - 1) * runif(n - 
        1))
    #Selecting the random sample using the random indices
    R_boot <- R[uniform, ]  # select random sample
    #getting the simulated metals mean
    mean_vect <- apply(R_boot, 2, mean)
    #Saving the means in the mean matrix
    mean_out[i_boot, ] <- mean_vect
    #Getting the covariance and standard deviations for the simutaltion
    cov_mat <- cov(R_boot)
    sd_vect <- sqrt(diag(cov_mat))
    #Creating the constraint matrix for this simulation
    Amat <-  cbind(rep(1,3),mean_vect,diag(1,nrow=3))
    #Creating a variable to store teh standard deviation of the portfolio 
    sd.P <- numeric(300)
    #Getting the returns for the risk-free asset
    mu.free <- (3 / 100 / 365)
    #creating a sequence for possible returns 
    mu.P <- seq(min(mean_vect) + 1e-04, 
        max(mean_vect) - 1e-04, length = 300)  #length.P)
    #Creating a weight matrix to save the positions (300 rows 3 columns)
    weights <- matrix(0, nrow = 300, 
        ncol = N)
    #fro each oof the possible returns
    for (i in 1:length(mu.P)) {
        #initial values
        bvec <-  c(1,mu.P[i],rep(0,3))
        #Solving portfolio for the target return
        result <- solve.QP(Dmat = 2 * 
            cov_mat, dvec = rep(0, N), 
            Amat = Amat, bvec = bvec, 
            meq = 2)
        #Saving the sd in the standard deviation matrix
        sd.P[i] <- sqrt(result$value)
        #Saving the weights in the weights vector
        weights[i, ] <- result$solution
    }
    #Solving for the sharpe ratio of this scenario
    sharpe <- (mu.P - mu.free)/sd.P
    #Indexing the highest sharpe ratio
    ind <- (sharpe == max(sharpe))
    #Saving the maximum sharpe's ratio for this simulation 
    out[i_boot, 1] <- sharpe[ind]
    #Storing the weights for the sharpe's ratio in the 
    #weights matrix
    w.T <- weights[ind, ]
    #Showing how returns would be for this weight in the actual
    sharpe_ACTUAL <- (w.T %*% mean_vect_ACTUAL - 
        mu.free)/sqrt(w.T %*% cov_mat_ACTUAL %*% 
        w.T)
    #Storing the returns for these weights 
    out[i_boot, 2] <- sharpe_ACTUAL
}
#Creating a data frame that shows what we would 
#have made with those weights in that simulation, What
#the simulation predicted we would have made and the difference
#Between the predicted and the actual returns
out_SHORT <- data.frame(actual = out[, 
    2], predicted = out[, 1], residuals = out[, 
    2] - out[, 1])
#Create a data moments for our predicted and actual returns
out_SUMMARY <- data_moments(as.matrix(out_SHORT))
#Make the table nice
#Storing the results of the actual returns
results <- out_SHORT
#Getting the minimum x & y for plotting
min_xy <- min(min(results$actual), min(results$predicted))
#Getting the maximum x & y for plotting
max_xy <- max(max(results$actual), max(results$predicted))
#Melting the results to create a faect plot
plot_melt <- melt(results, id.vars = "predicted")
#Abbing the max x-y and min x-y so facet plots have the same scale
p <- ggplot(results, aes(predicted, actual)) + 
    geom_point() + ggtitle("Actual vs. Predicted Sharpe's Ratios") + 
    geom_quantile(quantiles = c(0.01, 
        0.99)) + geom_quantile(quantiles = 0.5, 
    linetype = "longdash") + geom_density_2d(colour = "red")
ggplotly(p)
})
```