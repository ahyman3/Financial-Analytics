#Fitting a quantile regression of nickel on copper
fit.rq.nickel.copper <- rq(log(nickel.copper) ~
log(copper.vol), tau = taus, data = r_corr_vol)
#Creates a dataframe that has all the rolling calculations and all the raw data
#and the year of that piece of data
r_corr_vol <- merge(ALL.r, corr_r, vol_r, year)
#Number of columns in x
dim <- ncol(x)
#calculate the correlation of r, nut only include the the
#lower triangle of the correlation matrix because the resulting
#matrix will be c x c
corr_r <- cor(x)[lower.tri(diag(dim), diag = FALSE)]
#If the returns is positive, make a new matrix have a 1 in that place
#If the returns is negative, put a -1 in that place, otherwise a 0
direction <- ifelse(data.r > 0, 1, ifelse(data.r < 0, -1, 0))
#Adding .dir to the end of the column names in the dirextion data frame
colnames(direction) <- paste(colnames(direction), ".dir", sep = "")
#Making a vector of the dates
dates <- as.Date(data[-1,1], "%m/%d/%Y")
#Making dates characters
dates.chr <- as.character(dates)
#Combining all columns to make a values data frame
values <- cbind(data.r, size, direction)
#Creating tidy data frames
data.df <- data.frame(dates = dates, returns = data.r, size = size, direction = direction)
data.df.nd <- data.frame(dates = dates.chr, returns = data.r, size = size, direction = direction, stringsAsFactors = FALSE)
#Creating a time series object
data.xts <- na.omit(as.xts(values, dates))
#Making a zoo object
data.zr <- as.zooreg(data.xts)
returns <- data.xts
# PAGE: Market risk
#Function to use in roll apply
corr_rolling <- function(x){
#Number of columns in x
dim <- ncol(x)
#calculate the correlation of r, nut only include the the
#lower triangle of the correlation matrix because the resulting
#matrix will be c x c
corr_r <- cor(x)[lower.tri(diag(dim), diag = FALSE)]
#Return all teh correlations
return(corr_r)
}
#Function to use in the rollapply function
vol_rolling <- function(x){
#Importing the library for matrix statistics
library(matrixStats)
#calculate the standard deviation in the column
vol_r <- colSds(x)
#return the SDs
return(vol_r)
}
#making a matrix of only the returns (no dates)
ALL.r <- data.xts[, 1:3]
#Creating a window of 90 days
window <- 90
#apply the rolling correlation function on the returns data
corr_r <- rollapply(ALL.r, width = window, corr_rolling, align = "right", by.column = FALSE)
#giving column names to the correlation matrix that has the two metals being measured
colnames(corr_r) <- c("nickel.copper", "nickel.aluminium", "copper.aluminium")
#apply the rolling correlation function on the returns data
vol_r <- rollapply(ALL.r, width = window, vol_rolling, align = "right", by.column = FALSE)
#giving the column names to specify they are volatilities being measured
colnames(vol_r) <- c("nickel.vol", "copper.vol", "aluminium.vol")
#Creating a vector that has the dates for all rows
year <- format(index(corr_r), "%Y")
#Creates a dataframe that has all the rolling calculations and all the raw data
#and the year of that piece of data
r_corr_vol <- merge(ALL.r, corr_r, vol_r, year)
#importing library for quantile regression
library(quantreg)
#Creating a vector of quantiles from 0.05 to 0.95 in increments of 0.05
taus <- seq(0.05, 0.95, 0.05)
#Fitting a quantile regression of nickel on copper
fit.rq.nickel.copper <- rq(log(nickel.copper) ~
log(copper.vol), tau = taus, data = r_corr_vol)
#Fitting linear regression on copper and nickel
fit.lm.nickel.copper <- lm(log(nickel.copper) ~
log(copper.vol), data = r_corr_vol)
?summary
ni.cu.summary <- summary(fit.rq.nickel.copper,
se = "boot")
plot(ni.cu.summary)
plot(ni.cu.summary)
ni.cu.summary <- summary(fit.rq.nickel.copper,
se = "boot")
ni.cu.summary <- summary(fit.rq.nickel.copper,
se = "boot")
plot(ni.cu.summary)
View(data.xts)
View(data.xts)
##
#Title for the plot
title.chg <- "Metals Market Percent Changes"
#Plotting the returns for copper, nickel, and aluminum from -5 to 5
autoplot.zoo(data.xts[, 1:3]) + ggtitle(title.chg) +
ylim(-5, 5)
#plotting the magnitude of change for the metals
title2 <- "Metals Market Magnitude of Change"
autoplot.zoo(data.xts[, 4:6]) + ggtitle(title2) +
ylim(-5, 5)
autoplot.zoo(data.xts[, 4:6]) + ggtitle(title2) +
ylim(-.5, 5)
#Autocorrelation in returns for the metals
acf(coredata(data.xts[, 1:3]))  # returns
#Autocorrelation for magnitude of returns
acf(coredata(data.xts[, 4:6]))  # sizes
# making time series of nickel returns
one <- ts(data.df$returns.nickel)
# making time series of copper returns
two <- ts(data.df$returns.copper)
#Creating title for ccf plot
title.chg <- "Nickel vs. Copper"
#ccf for nickel and copper
ccf(one, two, main = title.chg, lag.max = 20,
xlab = "", ylab = "", ci.col = "red")
#creating other time series
two <- ts(two)
#Creating function for repetition
#provide the two vectors, title, lag, and color of confidence interval
run_ccf <- function(one, two, main = title.chg,
lag = 20, color = "red") {
#Stop the function if the vectors are not equal length
stopifnot(length(one) == length(two))
#create one time series
one <- ts(one)
#creating other time series
two <- ts(two)
#Running the cross correlation function
ccf(one, two, main = main, lag.max = lag,
xlab = "", ylab = "", ci.col = color)
# end run_ccf
}
title <- "nickel-copper"
run_ccf(one, two, main = title, lag = 20,
color = "red")
data.zr
#Running the ccf function for volatilities
# now for volatility (sizes)
#
one <- abs(data.zr[, 1])
two <- abs(data.zr[, 2])
title <- "Nickel-Copper: volatility"
run_ccf(one, two, main = title, lag = 20,
color = "red")
data.zr
#Running the ccf function for volatilities
# now for volatility (sizes)
#
one <- abs(data.zr[, 4])
two <- abs(data.zr[, 5])
title <- "Nickel-Copper: volatility"
run_ccf(one, two, main = title, lag = 20,
color = "red")
data.xts
#Data Moments for nickel and
data_moments <- function(data) {
library(moments)
library(matrixStats)
mean.r <- colMeans(data)
median.r <- colMedians(data)
sd.r <- colSds(data)
IQR.r <- colIQRs(data)
skewness.r <- skewness(data)
kurtosis.r <- kurtosis(data)
result <- data.frame(mean = mean.r,
median = median.r, std_dev = sd.r,
IQR = IQR.r, skewness = skewness.r,
kurtosis = kurtosis.r)
return(result)
}
# Run
answer <- data_moments(data.xts[, 4:9])
# Build pretty table
answer <- round(answer, 4)
knitr::kable(answer)
data.xts
#Average size in change of nickel
mean(data.xts[, 4])
##
#Returns of nickel
returns1 <- returns[, 1]
returns1
#Naming the column
colnames(returns1) <- "Returns"
# Expected Shortfall
ES.hist <- median(returns1[returns1 >
VaR.hist])
##
#Returns of nickel
returns1 <- returns[, 1]
#Naming the column
colnames(returns1) <- "Returns"
#Creating data frame with a column "Distribution" that states
#That this is the historical returns
returns1.df <- data.frame(Returns = returns1[,
1], Distribution = rep("Historical",
each = length(returns1)))
#95% confidence interval
alpha <- 0.95  # reactive({ifelse(input$alpha.q>1,0.99,ifelse(input$alpha.q<0,0.001,input$alpha.q))})
# finding the value that is at the accepted risk in the returns
VaR.hist <- quantile(returns1, alpha)
#Creating text for the plot that will say what the Value at risk is
VaR.text <- paste("Value at Risk =",
round(VaR.hist, 2))
# Expected Shortfall is mean of values greater than VaR
ES.hist <- mean(returns1[returns1 >
VaR.hist])
ES.text <- paste("Expected Shortfall =",
round(ES.hist, 2))
p <- ggplot(returns1.df, aes(x = Returns,
fill = Distribution)) + geom_density(alpha = 0.5) +
geom_vline(aes(xintercept = VaR.hist),
linetype = "dashed", size = 1,
color = "firebrick1") + geom_vline(aes(xintercept = ES.hist),
size = 1, color = "firebrick1") +
annotate("text", x = 2 + VaR.hist,
y = VaR.y * 1.05, label = VaR.text) +
annotate("text", x = 1.5 + ES.hist,
y = VaR.y * 1.1, label = ES.text) +
scale_fill_manual(values = "dodgerblue4")
# Determine the max y value of the
# desity plot.  This will be used to
# place the text above the plot
VaR.y <- max(density(returns1.df$Returns)$y)
# Expected Shortfall is mean of values greater than VaR
ES.hist <- mean(returns1[returns1 >
VaR.hist])
ES.text <- paste("Expected Shortfall =",
round(ES.hist, 2))
p <- ggplot(returns1.df, aes(x = Returns,
fill = Distribution)) + geom_density(alpha = 0.5) +
geom_vline(aes(xintercept = VaR.hist),
linetype = "dashed", size = 1,
color = "firebrick1") + geom_vline(aes(xintercept = ES.hist),
size = 1, color = "firebrick1") +
annotate("text", x = 2 + VaR.hist,
y = VaR.y * 1.05, label = VaR.text) +
annotate("text", x = 1.5 + ES.hist,
y = VaR.y * 1.1, label = ES.text) +
scale_fill_manual(values = "dodgerblue4")
ES.text <- paste("Expected Shortfall =",
round(ES.hist, 2))
p
#Creating a ggplot object with the returns being x and fill being the Distribution
p <- ggplot(returns1.df, aes(x = Returns,
fill = Distribution)) + geom_density(alpha = 0.5) + #Making it a density plot
geom_vline(aes(xintercept = VaR.hist),#Adding vertical line at the VaR
linetype = "dashed", size = 1,
#Adding a vertical line at the expected shortfall
color = "firebrick1") + geom_vline(aes(xintercept = ES.hist),
size = 1, color = "firebrick1") +
#adding the text to the plot
annotate("text", x = 3 + VaR.hist,
y = VaR.y * 1.05, label = VaR.text) +
annotate("text", x = 1.5 + ES.hist,
y = VaR.y * 1.1, label = ES.text) +
scale_fill_manual(values = "dodgerblue4")
#Showing the plot
p
#Creating a ggplot object with the returns being x and fill being the Distribution
p <- ggplot(returns1.df, aes(x = Returns,
fill = Distribution)) + geom_density(alpha = 0.5) + #Making it a density plot
geom_vline(aes(xintercept = VaR.hist),#Adding vertical line at the VaR
linetype = "dashed", size = 1,
#Adding a vertical line at the expected shortfall
color = "firebrick1") + geom_vline(aes(xintercept = ES.hist),
size = 1, color = "firebrick1") +
#adding the text to the plot
annotate("text", x = 4 + VaR.hist,
y = VaR.y * 1.05, label = VaR.text) +
annotate("text", x = 4 + ES.hist,
y = VaR.y * 1.1, label = ES.text) +
scale_fill_manual(values = "dodgerblue4")
#Showing the plot
p
#Creating a ggplot object with the returns being x and fill being the Distribution
p <- ggplot(returns1.df, aes(x = Returns,
fill = Distribution)) + geom_density(alpha = 0.5) + #Making it a density plot
geom_vline(aes(xintercept = VaR.hist),#Adding vertical line at the VaR
linetype = "dashed", size = 1,
#Adding a vertical line at the expected shortfall
color = "firebrick1") + geom_vline(aes(xintercept = ES.hist),
size = 1, color = "firebrick1") +
#adding the text to the plot
annotate("text", x = 4 + VaR.hist,
y = VaR.y * 1.05, label = VaR.text) +
annotate("text", x = 3.5 + ES.hist,
y = VaR.y * 1.1, label = ES.text) +
scale_fill_manual(values = "dodgerblue4")
#Showing the plot
p
knitr::opts_chunk$set(echo = TRUE)
#Function to use in roll apply
corr_rolling <- function(x){
#Number of columns in x
dim <- ncol(x)
#calculate the correlation of r, nut only include the the
#lower triangle of the correlation matrix because the resulting
#matrix will be c x c
corr_r <- cor(x)[lower.tri(diag(dim), diag = FALSE)]
#Return all teh correlations
return(corr_r)
}
#Function to use in the rollapply function
vol_rolling <- function(x){
#Importing the library for matrix statistics
library(matrixStats)
#calculate the standard deviation in the column
vol_r <- colSds(x)
#return the SDs
return(vol_r)
}
#making a matrix of only the returns (no dates)
ALL.r <- data.xts[, 1:3]
library(ggplot2)
library(flexdashboard)
library(shiny)
library(QRM)
library(qrmdata)
library(xts)
library(zoo)
library(psych)
library(matrixStats)
#Removes all objects in this environment to blank out for the dashboard
rm(list = ls())
#Reading in the data
data <- na.omit(read.csv(url("https://turing.manhattan.edu/~wfoote01/finalytics/data/metaldata.csv"), header = TRUE))
#Applies the log difference to the numeric columns in the data matrix and multiplies by 100 to get percentage
data.r <- apply(log(data[,-1]), 2, diff) * 100
#Finding the magnitude of the percent change with absolute value
size <- na.omit(abs(data.r))
#adds .size to the end of the column
colnames(size) <- paste(colnames(size), ".size", sep = "")
#If the returns is positive, make a new matrix have a 1 in that place
#If the returns is negative, put a -1 in that place, otherwise a 0
direction <- ifelse(data.r > 0, 1, ifelse(data.r < 0, -1, 0))
#Adding .dir to the end of the column names in the dirextion data frame
colnames(direction) <- paste(colnames(direction), ".dir", sep = "")
#Making a vector of the dates
dates <- as.Date(data[-1,1], "%m/%d/%Y")
#Making dates characters
dates.chr <- as.character(dates)
#Combining all columns to make a values data frame
values <- cbind(data.r, size, direction)
#Creating tidy data frames
data.df <- data.frame(dates = dates, returns = data.r, size = size, direction = direction)
data.df.nd <- data.frame(dates = dates.chr, returns = data.r, size = size, direction = direction, stringsAsFactors = FALSE)
#Creating a time series object
data.xts <- na.omit(as.xts(values, dates))
#Making a zoo object
data.zr <- as.zooreg(data.xts)
returns <- data.xts
#Function to use in roll apply
corr_rolling <- function(x){
#Number of columns in x
dim <- ncol(x)
#calculate the correlation of r, nut only include the the
#lower triangle of the correlation matrix because the resulting
#matrix will be c x c
corr_r <- cor(x)[lower.tri(diag(dim), diag = FALSE)]
#Return all teh correlations
return(corr_r)
}
#Function to use in the rollapply function
vol_rolling <- function(x){
#Importing the library for matrix statistics
library(matrixStats)
#calculate the standard deviation in the column
vol_r <- colSds(x)
#return the SDs
return(vol_r)
}
#making a matrix of only the returns (no dates)
ALL.r <- data.xts[, 1:3]
#Creating a window of 90 days
window <- 90
#apply the rolling correlation function on the returns data
corr_r <- rollapply(ALL.r, width = window, corr_rolling, align = "right", by.column = FALSE)
#giving column names to the correlation matrix that has the two metals being measured
colnames(corr_r) <- c("nickel.copper", "nickel.aluminium", "copper.aluminium")
#apply the rolling correlation function on the returns data
vol_r <- rollapply(ALL.r, width = window, vol_rolling, align = "right", by.column = FALSE)
#giving the column names to specify they are volatilities being measured
colnames(vol_r) <- c("nickel.vol", "copper.vol", "aluminium.vol")
#Creating a vector that has the dates for all rows
year <- format(index(corr_r), "%Y")
#Creates a dataframe that has all the rolling calculations and all the raw data
#and the year of that piece of data
r_corr_vol <- merge(ALL.r, corr_r, vol_r, year)
#importing library for quantile regression
library(quantreg)
#Creating a vector of quantiles from 0.05 to 0.95 in increments of 0.05
taus <- seq(0.05, 0.95, 0.05)
#Fitting a quantile regression of nickel on copper
fit.rq.nickel.copper <- rq(log(nickel.copper) ~
log(copper.vol), tau = taus, data = r_corr_vol)
#Fitting linear regression on copper and nickel
fit.lm.nickel.copper <- lm(log(nickel.copper) ~
log(copper.vol), data = r_corr_vol)
#Creating a summary of the quantile regreassion fit
ni.cu.summary <- summary(fit.rq.nickel.copper,
se = "boot")
#plotting the summary/quantile regression coefficients
plot(ni.cu.summary)
#Autocorrelation in returns for the metals
acf(coredata(data.xts[, 1:3]))  # returns
#Autocorrelation for magnitude of returns
acf(coredata(data.xts[, 4:6]))  # sizes
#Running the ccf function for volatilities
# now for volatility (sizes)
#
one <- abs(data.zr[, 4])
two <- abs(data.zr[, 5])
title <- "Nickel-Copper: volatility"
run_ccf(one, two, main = title, lag = 20,
color = "red")
#Creating function for repetition
#provide the two vectors, title, lag, and color of confidence interval
run_ccf <- function(one, two, main = title.chg,
lag = 20, color = "red") {
#Stop the function if the vectors are not equal length
stopifnot(length(one) == length(two))
#create one time series
one <- ts(one)
#creating other time series
two <- ts(two)
#Running the cross correlation function
ccf(one, two, main = main, lag.max = lag,
xlab = "", ylab = "", ci.col = color)
# end run_ccf
}
title <- "nickel-copper"
run_ccf(one, two, main = title, lag = 20,
color = "red")
#Running the ccf function for volatilities
# now for volatility (sizes)
#
one <- abs(data.zr[, 4])
two <- abs(data.zr[, 5])
title <- "Nickel-Copper: volatility"
run_ccf(one, two, main = title, lag = 20,
color = "red")
#Data Moments for nickel and
data_moments <- function(data) {
library(moments)
library(matrixStats)
mean.r <- colMeans(data)
median.r <- colMedians(data)
sd.r <- colSds(data)
IQR.r <- colIQRs(data)
skewness.r <- skewness(data)
kurtosis.r <- kurtosis(data)
result <- data.frame(mean = mean.r,
median = median.r, std_dev = sd.r,
IQR = IQR.r, skewness = skewness.r,
kurtosis = kurtosis.r)
return(result)
}
# Run data moments for the returns and sizes of all metals
answer <- data_moments(data.xts[, 1:6])
# Build pretty table with 4 digits
answer <- round(answer, 4)
#Knitting the table
knitr::kable(answer)
#Average size in change of nickel
mean(data.xts[, 4])
renderPlotly({
## Toleranvce for risk
alpha.tolerance <- reactive({ifelse(input$gpd_confint>1,0.99,ifelse(input$gpd_confint<0,0.001,input$gpd_confint))})
#getting the losses at that tolerance for risk
u <- quantile(loss.rf, alpha.tolerance(),
names = FALSE)
#Losses exceed out tolaerance by how much?
loss.excess <- loss.rf[loss.rf > u] - u
#Using the generalized pareto distribution to fit losses with
#thresholds determined by u
fit <- fit.GPD(loss.rf, threshold = u)
#getting the fitted shape
xi.hat <- fit$par.ests[["xi"]]
#getting the fitted scale
beta.hat <- fit$par.ests[["beta"]]
#saving the losses in the data variable
data <- loss.rf
#how often is loss greater than our tolerance
n.relative.excess <- length(loss.excess)/length(loss.rf)
#VaR for pareto
VaR.gpd <- u + (beta.hat/xi.hat) * (((1 -
alpha.tolerance())/n.relative.excess)^(-xi.hat) -
1)
#estimated shortfall for pareto
ES.gpd <- (VaR.gpd + beta.hat - xi.hat *
u)/(1 - xi.hat)
# Plot away
VaRgpd.text <- paste("GPD: Value at Risk =",
round(VaR.gpd, 2))
ESgpd.text <- paste("Expected Shortfall =",
round(ES.gpd, 2))
title.text <- paste(VaRgpd.text, ESgpd.text,
sep = " ")
loss.plot <- ggplot(loss.rf.df, aes(x = Loss,
fill = Distribution)) + geom_density(alpha = 0.2)
loss.plot <- loss.plot + geom_vline(aes(xintercept = VaR.gpd),
colour = "blue", linetype = "dashed",
size = 0.8)
loss.plot <- loss.plot + geom_vline(aes(xintercept = ES.gpd),
colour = "blue", size = 0.8)
#+ annotate('text', x = 300, y = 0.0075, label = VaRgpd.text, colour = 'blue') + annotate('text', x = 300, y = 0.005, label = ESgpd.text, colour = 'blue')
loss.plot <- loss.plot + xlim(0, 500) +
ggtitle(title.text)
ggplotly(loss.plot)
})
